{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "507aeafe",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from statsmodels.formula.api import ols\n",
    "from scipy.stats import gaussian_kde\n",
    "import scipy\n",
    "import scipy.sparse\n",
    "import patsy\n",
    "from statistics import median\n",
    "import bz2\n",
    "import math\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47c2229",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir= './'\n",
    "def sort_cols(test):\n",
    "    return(test.reindex(sorted(test.columns), axis=1))\n",
    "frames = {}\n",
    "for year in [2004,2005,2006]:\n",
    "    fil = \"./data/\" + \"pandas-frames.\" + str(year) + \".pickle.bz2\"\n",
    "frames.update(pd.read_pickle(fil))\n",
    "for x in frames:\n",
    "    frames[x]= sort_cols(frames[x])\n",
    "covariance = {}\n",
    "for year in [2003,2004,2005,2006]:\n",
    "    fil =\"./data/\" + \"covariance.\" + str(year) + \".pickle.bz2\"\n",
    "covariance.update(pd.read_pickle(fil))\n",
    "industry_factors= ['AERODEF', 'AIRLINES', 'ALUMSTEL', 'APPAREL', 'AUTO',\n",
    "'BANKS','BEVTOB', 'BIOLIFE', 'BLDGPROD','CHEM', 'CNSTENG',\n",
    "'CNSTMACH', 'CNSTMATL', 'COMMEQP', 'COMPELEC',\n",
    "'COMSVCS', 'CONGLOM', 'CONTAINR', 'DISTRIB',\n",
    "'DIVFIN', 'ELECEQP', 'ELECUTIL', 'FOODPROD', 'FOODRET', 'GASUTIL',\n",
    "'HLTHEQP', 'HLTHSVCS', 'HOMEBLDG', 'HOUSEDUR','INDMACH', 'INSURNCE',\n",
    "'INTERNET',\n",
    "'LEISPROD', 'LEISSVCS', 'LIFEINS', 'MEDIA', 'MGDHLTH','MULTUTIL',\n",
    "'OILGSCON', 'OILGSDRL', 'OILGSEQP', 'OILGSEXP',\n",
    "'PAPER', 'PHARMA', 'PRECMTLS','PSNLPROD','REALEST',\n",
    "'RESTAUR', 'ROADRAIL','SEMICOND', 'SEMIEQP','SOFTWARE',\n",
    "'SPLTYRET', 'SPTYCHEM', 'SPTYSTOR', 'TELECOM', 'TRADECO', 'TRANSPRT',\n",
    "'WIRELESS']\n",
    "style_factors= ['BETA','SIZE','MOMENTUM','VALUE','LEVERAGE','LIQUIDTY']\n",
    "def wins(x,a,b):\n",
    "    return(np.where(x <= a,a, np.where(x >= b, b, x)))\n",
    "def clean_nas(df):\n",
    "    numeric_columns= df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    for numeric_column in numeric_columns:\n",
    "        df[numeric_column]= np.nan_to_num(df[numeric_column])\n",
    "        return df\n",
    "def density_plot(data, title):\n",
    "    density= gaussian_kde(data)\n",
    "    xs = np.linspace(np.min(data),np.max(data),200)\n",
    "    density.covariance_factor= lambda : .25\n",
    "    density._compute_covariance()\n",
    "    plt.plot(xs,density(xs))\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    test = frames['20040102']\n",
    "    density_plot(test['Ret'], 'Daily return pre-winsorization')\n",
    "    density_plot(wins(test['Ret'],-0.2,0.2), 'Daily return winsorized')\n",
    "    D = (test['SpecRisk'] / (100 * math.sqrt(252))) ** 2\n",
    "    density_plot(np.sqrt(D), 'SpecRisk')\n",
    "def get_estu(df):\n",
    "    \"\"\"Estimation universe definition\"\"\"\n",
    "    estu = df.loc[df.IssuerMarketCap > 1e9].copy(deep=True)\n",
    "    return estu\n",
    "def colnames(X):\n",
    "    \"\"\" return names of columns, for DataFrame or DesignMatrix \"\"\"\n",
    "    if(type(X)== patsy.design_info.DesignMatrix):\n",
    "        return(X.design_info.column_names)\n",
    "    if(type(X)== pandas.core.frame.DataFrame):\n",
    "        return(X.columns.tolist())\n",
    "    return(None)\n",
    "def diagonal_factor_cov(date, X):\n",
    "    \"\"\"Factor covariance matrix, ignoring off-diagonal for simplicity\"\"\"\n",
    "    cv = covariance[date]\n",
    "    k = np.shape(X)[1]\n",
    "    Fm = np.zeros([k,k])\n",
    "    for j in range(0,k):\n",
    "        fac = colnames(X)[j]\n",
    "    Fm[j,j]= (0.01**2) * cv.loc[(cv.Factor1==fac) & (cv.Factor2==fac),\"VarCovar\"]\n",
    "    return(Fm)\n",
    "def risk_exposures(estu):\n",
    "    \"\"\"Exposure matrix for risk factors, usually called X in class\"\"\"\n",
    "    L = [\"0\"]\n",
    "    L.extend(style_factors)\n",
    "    L.extend(industry_factors)\n",
    "    my_formula= \" + \".join(L)\n",
    "    return patsy.dmatrix(my_formula, data = estu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f007bf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_date= '20040102'\n",
    "# estu = estimation universe\n",
    "estu = get_estu(frames[my_date])\n",
    "estu['Ret']= wins(estu['Ret'],-0.25, 0.25)\n",
    "rske = risk_exposures(estu)\n",
    "F = diagonal_factor_cov(my_date, rske)\n",
    "X = np.asarray(rske)\n",
    "D = np.asarray( (estu['SpecRisk'] / (100 * math.sqrt(252))) ** 2 )\n",
    "kappa= 1e-5\n",
    "candidate_alphas= [\n",
    "'STREVRSL', 'LTREVRSL', 'INDMOM',\n",
    "'EARNQLTY', 'EARNYILD', 'MGMTQLTY', 'PROFIT', 'SEASON', 'SENTMT']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f6098b",
   "metadata": {},
   "source": [
    "Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567bee37",
   "metadata": {},
   "outputs": [],
   "source": [
    "for date in frames:\n",
    "    estu = get_estu(frames[date])\n",
    "    estu['Ret']= wins(estu['Ret'],-0.25, 0.25)\n",
    "    X = np.asarray(risk_exposures(estu))\n",
    "    Ret = np.asarray(estu['Ret'])\n",
    "    X_plus= np.linalg.pinv(X)\n",
    "    risk_model_explanation= X @ (X_plus @ Ret)\n",
    "    Y = Ret- risk_model_explanation\n",
    "    estu['Y']= Y\n",
    "    frames[date]= estu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064ccea8",
   "metadata": {},
   "source": [
    "Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9245a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 2\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "sorted_dates= sorted(list(frames.keys()))\n",
    "split_point= int(len(sorted_dates) * 0.7)\n",
    "train_dates= sorted_dates[:split_point]\n",
    "test_dates= sorted_dates[split_point:]\n",
    "print(f\"Training on {len(train_dates)} dates. Testing on {len(test_dates)} dates.\")\n",
    "train_panel= pd.concat([frames[d] for d in train_dates])\n",
    "test_panel= pd.concat([frames[d] for d in test_dates]) # The Vault\n",
    "train_clean= train_panel.dropna(subset=candidate_alphas + ['Y'])\n",
    "X_train= train_clean[candidate_alphas]\n",
    "y_train= train_clean['Y']\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "print(\"\\nTraining Linear Model (Lasso)...\")\n",
    "lasso = LassoCV(cv=tscv, random_state=42).fit(X_train, y_train)\n",
    "lasso_score= lasso.score(X_train, y_train) # R^2 score\n",
    "print(f\"Lasso Best Alpha: {lasso.alpha_:.6f}\")\n",
    "print(\"\\nTraining Random Forest...\")\n",
    "rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "rf_params= {\n",
    "'n_estimators': [50, 100],\n",
    "'max_depth': [3, 5],\n",
    "'min_samples_leaf': [100]\n",
    "}\n",
    "rf_grid= GridSearchCV(rf, rf_params, cv=tscv, scoring='neg_mean_squared_error')\n",
    "rf_grid.fit(X_train, y_train)\n",
    "print(f\"Best RF Params: {rf_grid.best_params_}\")\n",
    "print(\"\\nTraining Gradient Boosting...\")\n",
    "gb= GradientBoostingRegressor(random_state=42)\n",
    "gb_params= {\n",
    "'n_estimators': [50, 100],\n",
    "'learning_rate': [0.01, 0.1], # Key parameter for Boosting\n",
    "'max_depth': [3], # Boosting prefers shallow trees\n",
    "'min_samples_leaf': [100]\n",
    "}\n",
    "gb_grid= GridSearchCV(gb, gb_params, cv=tscv, scoring='neg_mean_squared_error')\n",
    "gb_grid.fit(X_train, y_train)\n",
    "print(f\"Best GB Params: {gb_grid.best_params_}\")\n",
    "print(\"\\n--- Model Comparison (MSE - Lower is Better) ---\")\n",
    "lasso_mse= mean_squared_error(y_train, lasso.predict(X_train)) # Simple MSE for Lasso\n",
    "rf_mse= -rf_grid.best_score_\n",
    "gb_mse= -gb_grid.best_score_\n",
    "print(f\"Lasso MSE: {lasso_mse:.6f}\")\n",
    "print(f\"Random Forest CV MSE: {rf_mse:.6f}\")\n",
    "print(f\"Gradient Boosting CV MSE: {gb_mse:.6f}\")\n",
    "best_score= min(lasso_mse, rf_mse, gb_mse)\n",
    "if best_score== lasso_mse:\n",
    "    print(\"\\nWINNER: Lasso (Linear)\")\n",
    "    final_model= lasso\n",
    "elif best_score== rf_mse:\n",
    "    print(\"\\nWINNER: Random Forest\")\n",
    "    final_model= rf_grid.best_estimator_\n",
    "else:\n",
    "    print(\"\\nWINNER: Gradient Boosting\")\n",
    "    final_model= gb_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5700643c",
   "metadata": {},
   "source": [
    "Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f793acda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def woodbury_inverse(X, F, D, eps=1e-8):\n",
    "    n, p = X.shape\n",
    "    D = np.maximum(np.nan_to_num(D, nan=eps, posinf=eps, neginf=eps), eps)\n",
    "    X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    F = np.nan_to_num(F, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    D_inv= 1.0 / D\n",
    "    F_reg= F + eps * np.eye(p)\n",
    "    try:\n",
    "        F_inv= np.linalg.inv(F_reg)\n",
    "    except np.linalg.LinAlgError:\n",
    "        F_inv= np.linalg.pinv(F_reg)\n",
    "        XtDinv = X.T * D_inv\n",
    "        XtDinvX = XtDinv @ X\n",
    "        middle = F_inv + XtDinvX\n",
    "    try:\n",
    "        middle_inv= np.linalg.inv(middle)\n",
    "    except np.linalg.LinAlgError:\n",
    "        middle_inv= np.linalg.pinv(middle)\n",
    "        DinvX = D_inv[:, None] * X\n",
    "    def apply(v):\n",
    "        v = np.nan_to_num(v, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        term1 = D_inv * v\n",
    "        term2 = DinvX @ (middle_inv @ (X.T @ term1))\n",
    "        return term1- term2\n",
    "    return apply"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa9b3c1",
   "metadata": {},
   "source": [
    "Problem 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f702ff23",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_risk= 0.01\n",
    "position_limit= 0.05\n",
    "min_variance= 1e-6\n",
    "initial_capital= 1_000_000\n",
    "backtest_results= []\n",
    "for date in test_dates:\n",
    "    if date not in frames:\n",
    "        continue\n",
    "    try:\n",
    "        estu = frames[date]\n",
    "        if len(estu) < 50:\n",
    "            continue\n",
    "        X_alpha= estu[candidate_alphas].fillna(0)\n",
    "        alpha= final_model.predict(X_alpha)\n",
    "        alpha= np.nan_to_num(alpha, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        if np.sum(np.abs(alpha)) < 1e-10:\n",
    "            continue\n",
    "        X_risk= np.asarray(risk_exposures(estu))\n",
    "        F_risk= diagonal_factor_cov(date, risk_exposures(estu))\n",
    "        D_risk= np.asarray((estu['SpecRisk'] / (100 * math.sqrt(252))) ** 2)\n",
    "        sigma_inv= woodbury_inverse(X_risk, F_risk, D_risk, min_variance)\n",
    "        h = (1.0 / lambda_risk) * sigma_inv(alpha)\n",
    "        if np.any(np.isnan(h)) or np.any(np.isinf(h)):\n",
    "            continue\n",
    "        h = h- np.mean(h)\n",
    "        h = wins(h,\n",
    "        -position_limit, position_limit)\n",
    "        gross = np.sum(np.abs(h))\n",
    "        if gross <= 1e-10:\n",
    "            continue\n",
    "        h /= gross\n",
    "        ret = np.nan_to_num(estu['Ret'].values, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        pnl= np.dot(h, ret)\n",
    "        hX = h @ X_risk\n",
    "        factor_var= float(hX @ F_risk @ hX.T)\n",
    "        idio_var= float(np.dot(h * h, D_risk))\n",
    "        total_var= factor_var + idio_var\n",
    "        total_risk= math.sqrt(max(0, total_var))\n",
    "        factor_risk= math.sqrt(max(0, factor_var))\n",
    "        idio_risk= math.sqrt(max(0, idio_var))\n",
    "        backtest_results.append({\n",
    "            'date': date,\n",
    "            'pnl': pnl,\n",
    "            'long_mv': float(np.sum(h[h > 0])),\n",
    "            'short_mv': float(np.abs(np.sum(h[h < 0]))),\n",
    "            'daily_risk_dollars': total_risk * initial_capital,\n",
    "            'daily_factor_risk_dollars': factor_risk * initial_capital,\n",
    "            'daily_idio_risk_dollars': idio_risk * initial_capital,\n",
    "            'idio_risk_pct': 100 * idio_risk / total_risk if total_risk > 0 else\n",
    "            'num_positions': int(np.sum(np.abs(h) > 1e-6))\n",
    "            })\n",
    "    except Exception:\n",
    "        continue\n",
    "results_df= pd.DataFrame(backtest_results)\n",
    "results_df['cumulative_pnl']= results_df['pnl'].cumsum()\n",
    "results_df['trading_day']= np.arange(len(results_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c25325f",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_pnl= results_df['pnl'].sum()\n",
    "avg_daily_pnl= results_df['pnl'].mean()\n",
    "vol_daily_pnl= results_df['pnl'].std()\n",
    "sharpe= avg_daily_pnl / vol_daily_pnl * np.sqrt(252)\n",
    "total_pnl, sharpe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd24b9f4",
   "metadata": {},
   "source": [
    "Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030c281b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 12))\n",
    "axes[0, 0].plot(results_df['trading_day'], results_df['long_mv'])\n",
    "axes[0, 0].fill_between(results_df['trading_day'], 0, results_df['long_mv'])\n",
    "axes[0, 0].set_title('Long Market Value')\n",
    "axes[0, 1].plot(results_df['trading_day'], results_df['short_mv'])\n",
    "axes[0, 1].fill_between(results_df['trading_day'], 0, results_df['short_mv'])\n",
    "axes[0, 1].set_title('Short Market Value')\n",
    "axes[1, 0].plot(results_df['trading_day'], results_df['cumulative_pnl'])\n",
    "axes[1, 0].fill_between(results_df['trading_day'], 0, results_df['cumulative_pnl'])\n",
    "axes[1, 0].axhline(0)\n",
    "axes[1, 0].set_title('Cumulative Profit')\n",
    "axes[1, 1].plot(results_df['trading_day'], results_df['daily_risk_dollars'])\n",
    "axes[1, 1].fill_between(results_df['trading_day'], 0, results_df['daily_risk_dollars'])\n",
    "axes[1, 1].set_title('Daily Portfolio Risk (Dollars)')\n",
    "axes[2, 0].plot(results_df['trading_day'], results_df['idio_risk_pct'])\n",
    "axes[2, 0].fill_between(results_df['trading_day'], 0, results_df['idio_risk_pct'])\n",
    "axes[2, 0].axhline(50)\n",
    "axes[2, 0].set_ylim(0, 100)\n",
    "axes[2, 0].set_title('Percent of Risk that is Idiosyncratic')\n",
    "axes[2, 1].plot(results_df['trading_day'], results_df['daily_factor_risk_dollars'])\n",
    "axes[2, 1].plot(results_df['trading_day'], results_df['daily_idio_risk_dollars'])\n",
    "axes[2, 1].set_title('Risk Decomposition (Dollars)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
